{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Contents\n",
    "\n",
    "1. Importing necessary libraries\n",
    "2. Authenticating with Hugging Face Hub\n",
    "3. Loading the SQuAD dataset\n",
    "4. Loading the DistilGPT-2 tokenizer\n",
    "5. Preprocessing the dataset\n",
    "6. Tokenizing the dataset using the tokenizer\n",
    "7. Grouping Tokenized Text\n",
    "8. Get train and evaluation datasets\n",
    "9. Fine-tuning the model\n",
    "10. Evaluating the fine-tuned model\n",
    "11. Push model to Hugging Face Hub\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"c1\"></a> <br>\n",
    "### 1) Importing necessary libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"c2\"></a> <br>\n",
    "### 2) Authenticating with Hugging Face Hub\n",
    "You gain access to private repositories and the ability to **push**, **pull**, and **manage models** on the *Hugging Face Hub* directly from your notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8245b9184d8047fb8bc5dc734b067fb1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"c3\"></a> <br>\n",
    "### 3) Loading the SQuAD dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 87599\n    })\n    validation: Dataset({\n        features: ['id', 'title', 'context', 'question', 'answers'],\n        num_rows: 10570\n    })\n})"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"squad\")\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"c4\"></a> <br>\n",
    "### 4) Loading the DistilGPT-2 tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'distilgpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "special_tokens = tokenizer.special_tokens_map\n",
    "print(special_tokens)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"c5\"></a> <br>\n",
    "### 5) Preprocessing the dataset\n",
    "Since we are going to use `distilgpt2` as our tokenizer, we should add the corresponding special tokens to the dataset. The special tokens are added to the dataset using the `map` function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "def add_end_token_to_question(input_dict):\n",
    "    input_dict['question'] += special_tokens['bos_token']\n",
    "    return input_dict\n",
    "\n",
    "dataset = dataset.remove_columns(['id', 'title', 'context', 'answers'])\n",
    "dataset = dataset.map(add_end_token_to_question)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"c6\"></a> <br>\n",
    "### 6) Tokenizing the dataset using the tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 87599\n    })\n    validation: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 10570\n    })\n})"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(input_dict):\n",
    "    return tokenizer(input_dict['question'], truncation=True)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=['question'])\n",
    "tokenized_dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"c7\"></a> <br>\n",
    "### 7) Grouping Tokenized Text\n",
    "\n",
    "The grouping tokenized text process involves dividing a tokenized text into fixed-length blocks or chunks to efficiently process large datasets during NLP tasks. By splitting the tokenized sequence into smaller segments, each of equal size, it becomes easier to handle and process the data in parallel, making it ideal for tasks like language modeling and text generation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "max_block_length = 128\n",
    "\n",
    "def divide_tokenized_text(tokenized_text_dict, block_size):\n",
    "    \"\"\"\n",
    "    Divides the tokenized text in the examples into fixed-length blocks of size block_size.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    tokenized_text_dict: dict\n",
    "        A dictionary containing tokenized text as values for different keys.\n",
    "\n",
    "    block_size: int\n",
    "        The desired length of each tokenized block.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "        dict: A dictionary with tokenized text divided into fixed-length blocks.\n",
    "    \"\"\"\n",
    "    concatenated_examples = {k: sum(tokenized_text_dict[k], []) for k in tokenized_text_dict.keys()}\n",
    "    total_length = len(concatenated_examples[list(tokenized_text_dict.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "\n",
    "    result = {\n",
    "        k: [t[i: i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "\n",
    "    result['labels'] = result['input_ids'].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "lm_dataset = tokenized_dataset.map(\n",
    "    lambda tokenized_text_dict: divide_tokenized_text(tokenized_text_dict, max_block_length),\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=4,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"c8\"></a> <br>\n",
    "### 8) Get train and evaluation datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "train_dataset = lm_dataset['train'].shuffle(seed=42).select(range(100))\n",
    "eval_dataset = lm_dataset['validation'].shuffle(seed=42).select(range(100))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"c9\"></a> <br>\n",
    "### 9) Fine-tuning the model\n",
    "\n",
    "The training process is controlled by the TrainingArguments, where we define hyperparameters like the learning rate and weight decay. The model is trained on a question-answering dataset, divided into training and evaluation sets (`train_dataset` and `eval_dataset`). During training, the model's parameters are optimized to predict answers for given questions, making it capable of providing accurate responses to queries.\n",
    "\n",
    "Also, To ensure the model's compatibility with the tokenization process, we add a special '[PAD]' token to the tokenizer.\n",
    "\n",
    "By running this section of code, you will have a fine-tuned GPT-2 model optimized for question answering. **(SQuAD)**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f'./{model_checkpoint}-squad',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False, # Change to True to push the model to the Hub\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"c10\"></a> <br>\n",
    "### 10) Evaluating the fine-tuned model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 1/13 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 159.82\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f'Perplexity: {math.exp(eval_results[\"eval_loss\"]):.2f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"c11\"></a> <br>\n",
    "### 11) Push model to Hugging Face Hub"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('gpt2-squad')\n",
    "model.push_to_hub('gpt2-squad')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
